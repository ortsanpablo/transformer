Building the environment:

conda create --name opitz-transformer python=3.7
conda activate opitz-transformer
conda install pytorch torchvision torchaudio cudatoolkit=11.3 -c pytorch
pip install torchtext
conda install -c conda-forge spacy
python -m spacy download de_core_news_sm
python -m spacy download en_core_web_sm
pip install -U matplotlib

alternatively: install from environment.yml It will fail when catching the spacy languages. After that:
conda activate opitz-transformer
python -m spacy download de_core_news_sm
python -m spacy download en_core_web_sm
pip install -U matplotlib

# FNet Training
Time = 16:36 - 17:48 100 Epochs ~72minutes
Epoch Time: 43 Seconds

Parameters:
# model parameter setting
batch_size = 128
max_len = 512
d_model = 512
n_layers = 6
n_heads = 8
ffn_hidden = 2048
drop_prob = 0.1

# optimizer parameter setting
init_lr = 1e-5
factor = 0.9
adam_eps = 5e-9
patience = 10
warmup = 10
epoch = 100
clip = 1.0
weight_decay = 5e-4
inf = float('inf')


GPU: RTX 3060ti
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 511.65       Driver Version: 511.65       CUDA Version: 11.6     |
|-------------------------------+----------------------+----------------------+
| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce ... WDDM  | 00000000:01:00.0  On |                  N/A |
| 60%   53C    P8    25W / 200W |   7773MiB /  8192MiB |      4%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
Peak Memory during 1 Epoch: 1323192 bytes according to tracemalloc

ATTention Training:
Time: 18:43 -
0m47sPeak Memory:1403347

Parameters:
# model parameter setting
batch_size = 128
max_len = 512
d_model = 512
n_layers = 6
n_heads = 8
ffn_hidden = 2048
drop_prob = 0.1

# optimizer parameter setting
init_lr = 1e-5
factor = 0.9
adam_eps = 5e-9
patience = 10
warmup = 10
epoch = 100
clip = 1.0
weight_decay = 5e-4
inf = float('inf')

during training
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 511.65       Driver Version: 511.65       CUDA Version: 11.6     |
|-------------------------------+----------------------+----------------------+
| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce ... WDDM  | 00000000:01:00.0  On |                  N/A |
| 55%   61C    P2   166W / 200W |   7615MiB /  8192MiB |     95%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
during evaluation
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 511.65       Driver Version: 511.65       CUDA Version: 11.6     |
|-------------------------------+----------------------+----------------------+
| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce ... WDDM  | 00000000:01:00.0  On |                  N/A |
| 62%   54C    P3    37W / 200W |   7790MiB /  8192MiB |      2%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+


----------------------------------------------------------------------------------------
Low Resource ATTENTION: GTX 970 Ti 4Gb(3.5 + 0.5) VRAM
Parameters:
# model parameter setting
batch_size = 128
max_len = 512
d_model = 512
n_layers = 6
n_heads = 8
ffn_hidden = 2048
drop_prob = 0.1

# optimizer parameter setting
init_lr = 1e-5
factor = 0.9
adam_eps = 5e-9
patience = 10
warmup = 10
epoch = 100
clip = 1.0
weight_decay = 5e-4
inf = float('inf')

This resolves in a out of memory, which was to be expected. This exact setup peaked at 5.5GB allocated VRAM.

# model parameter setting
batch_size = 64
max_len = 64
d_model = 512
n_layers = 6
n_heads = 8
ffn_hidden = 2048
drop_prob = 0.1

# optimizer parameter setting
init_lr = 1e-5
factor = 0.9
adam_eps = 5e-9
patience = 10
warmup = 10
epoch = 100
clip = 1.0
weight_decay = 5e-4
inf = float('inf')

OOM

--------------------------------------------------------------------
Low Resource FNet: GTX 970 Ti 4Gb(3.5 + 0.5) VRAM

# model parameter setting
batch_size = 64
max_len = 64
d_model = 512
n_layers = 6
n_heads = 8
ffn_hidden = 2048
drop_prob = 0.1

# optimizer parameter setting
init_lr = 1e-5
factor = 0.9
adam_eps = 5e-9
patience = 10
warmup = 10
epoch = 100
clip = 1.0
weight_decay = 5e-4
inf = float('inf')
OOM in 21st epoch, produces a model with about 8 BLEU

----------------------------------------------------------
# model parameter setting
batch_size = 128
max_len = 512
d_model = 256
n_layers = 6
n_heads = 8
ffn_hidden = 1024
drop_prob = 0.1

Both ATT and FNet OOM

lowres2 Experiments:
# model parameter setting
batch_size = 128
max_len = 512
d_model = 256
n_layers = 4
n_heads = 8
ffn_hidden = 1024
drop_prob = 0.1

lowres3 Experiments:
# model parameter setting
batch_size = 128
max_len = 512
d_model = 256
n_layers = 4
n_heads = 8
ffn_hidden = 2048
drop_prob = 0.1

ATT OOM
FNet BLEU 20